{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "Hi! Welcome again! Welcome to the complete tutorial for [NER or Named Entity Recognition](https://www.youtube.com/watch?v=MmgjhvOSd-E) using different ways. So for all who do not know what it is, let me explain it to you clearly. \n",
    "**It is whatever you want it to be** .Confused? I know. So Named Entity Recognition is a type of Language specific task which extracts the `USEFUL` entities from the sentence. And how do you decide what is useful. Well! You decide.\n",
    "\n",
    "Let us take an example:\n",
    "\n",
    "**Eminem has the the highest number of Top charts ever in Billboards top-100 but Kanye has more than 25 Grammies**. \n",
    "\n",
    "What do you propose is useful? Well! to me, (Eminem,person), (Billboard,organisation) but for some, it can be (Kanye,person) and (25,numerical). There can be lots of useful entities in the sentence which are of very common use. But we can also extract our own. So in this tutorial, we will be training our our NER system using [SpaCy](https://spacy.io/), [Keras](https://keras.io/) and [BERT](https://www.youtube.com/watch?v=TQQlZhbC5ps)\n",
    "\n",
    "For those who are wondering what `SpaCy` and `BERT` are, in simplest terms, `SpaCy` is an open source package which is dedicated to language specific tasks and is directed to serve a broad audience.\n",
    "\n",
    "On the other hand, **BERT** is type of [Transformer](https://www.youtube.com/watch?v=EXNBy8G43MM) architecture proposed by researchers at Google. It is a base architecture for lots of new, very smart language NLP models out there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "We have a unique problem at our disposal. Let us suppose that we have thousands of legal documents, say rent agreement and we have to get some specific details from each and every document like: Buyer, seller, Date of Agreement, Amount etc. So doing this manually, nah! you wouldn't be here.\n",
    "\n",
    "What we can do is we can make a regular expression that covers each of the entities so that we can extract. But hey!! those of us who have used `re`, we know the pain of building all those expressions after hours of work and failing for many use cases. And most of all, what if we have 10 names, 20 dates and 30 payments in a single document. How would you know that which one is actual Seller, Buyer and so on?\n",
    "\n",
    "# NER to the Rescue:\n",
    "So basically, we have models which can learn in a specific way we want it to learn. The basic models can give us very default findings like [Staanford NER Model](https://nlp.stanford.edu/software/CRF-NER.shtml) and SpaCy's NER model which can tell date, person,org etc BUT! the problem of telling who is buyer and who is seller still remains. One benifit of these pretrained models are that they already what Language, its' Grammar and other things about a language looks like (Neural Networks man! they just KNOW). So we just need to teach them how to find the specific things from a sentence. It is just like paying \n",
    "[Attention](https://ai.stackexchange.com/questions/21389/what-is-the-intuition-behind-the-attention-mechanism) and [Self Attention](https://www.youtube.com/watch?v=yGTUuEx3GkA) for learning new things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution\n",
    "We'll be looking at a few solutions to do this kind of work. Just because we have very scarce data, we won't be able to produce any good results if we try to train the model from scratch but we'll look at the different methodologies. This tutorial is just to give you a brief idea about NER and how can you fine tune existing and build your own model.\n",
    "\n",
    "1. Retraining SpaCy on top of existing\n",
    "2. Retraining BERT \n",
    "3. Building a model from scratch using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and install\n",
    "Some of the imports are essential and some are for your own good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow_hub # ELMO Embeddings\n",
    "# !pip install tensorflow-addon # Add ons apart from tensorflow core modules\n",
    "# !pip install git+https://www.github.com/keras-team/keras-contrib.git # CRF Layer Implementation\n",
    "# !pip install tf2crf # for CRF layer (you'll know)\n",
    "\n",
    "# !pip install python-docx # to load doc file\n",
    "# !pip install textract # to extract text from doc\n",
    "# !pip install num2words # change number to words: 10: ten\n",
    "# !pip install inflect # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
    "import re\n",
    "import docx\n",
    "import textract\n",
    "\n",
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy import displacy\n",
    "\n",
    "import calendar\n",
    "import datetime\n",
    "from num2words import num2words\n",
    "import inflect\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Embedding, Dense, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tf2crf import ModelWithCRFLoss, CRF, ModelWithCRFLossDSCLoss\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defaults\n",
    "Set defaults paths and seeds so that you can get results same everytime you train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = './data/Training_data/'\n",
    "train_labels=pd.read_csv('./data/TrainingTestSet.csv')\n",
    "val_labels=pd.read_csv('./data/ValidationSet.csv')\n",
    "train_text_dir='./data/Training_data/'\n",
    "val_text_dir='./data/Validation_data/'\n",
    "\n",
    "SEED = 13\n",
    "ver_37 = float(sys.version[:3])>=3.7 # get the python version\n",
    "\n",
    "engine = inflect.engine()\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning) # Supress some warnings which are not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProProcessing - Tagging Helpers\n",
    "What is more important than building the model itself? Preprocessing the data. A model with bad data is worse than no model at all just like no money is still better than debts. Every AI and even simple **DATA** related task need some way of preprocessing for sure and preprocessing tasks change from task to task.\n",
    "\n",
    "So in our task, we have to extract the related entities from the document. Always pay attention to your enemies and your data. If you look at the data, you'll see that our text have different formats than the one given in  training CSV like tyhe dates are given in CSV are `day.month.year: 08.09.2020` but in the text it can be `27th Oct., 2020` or `seventh of jan two thousand ten`. I can not explain the whole process but looking at the comments and the docstring of methods, you'll get the idea what and why have I tried to do what I have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tex =  {1:'one',2:'two','3':'three',4:'four',5:'five',6:'six',7:'seven',8:'eight',9:'nine',10:'ten',11:'eleven',12:'twelve',\n",
    "             13:'thirteen',14:'fourteen',15:'fifteen',16:'sixteen',17:'seventeen',18:'eighteen',19:'nineteen',20:'twenty'}\n",
    "\n",
    "al_day_alnum_day = {'first':'1st','second':'2nd','third':'3rd','fourth':'4th','fifth':'5th','sixth':'6th','seventh':'7th','eighth':'8th',\n",
    "                    'nineth':'9th','tenth':'10th','eleventh':'11th','twelfth':'12th','thirteenth':'13th','fourteenth':'14th',\n",
    "                    'fifteenth':'15th','sixteenth':'16th','seventeeth':'17th','eighteenth':'18th','nineteenth':'19th','twentieth':'20th',\n",
    "                    'twenty first': '21st','twenty second': '22nd','twenty third': '23rd','twenty fourth': '24th', 'twenty fifth': '25th',\n",
    "                    'twenty sixth': '26th','twenty seventh': '27th','twenty eighth': '28th','twenty nineth': '29th','thirtieth':'30th',\n",
    "                    'thirty first':'31st'}\n",
    "\n",
    "alnum_day_al_day =  dict((v,k) for k,v in al_day_alnum_day.items())\n",
    "\n",
    "alnum_day_num_day = {'first':'01','second':'02','third':'03','fourth':'04','fifth':'05','sixth':'06','seventh':'07','eighth':'08',\n",
    "                     'nineth':'09','tenth':'10','eleventh':'11','twelfth':'12','thirteenth':'13','fourteenth':'14','fifteenth':'15',\n",
    "                     'sixteenth':'16','seventeeth':'17','eighteenth':'18','nineteenth':'19','twentieth':'20','twenty first': '21',\n",
    "                     'twenty second': '22','twenty third': '23','twenty fourth': '24', 'twenty fifth': '25', 'twenty sixth': '26',\n",
    "                     'twenty seventh': '21','twenty eighth': '28','twenty nineth': '29','thirtieth':'30th','thirty first':'31'}\n",
    "        \n",
    "num_day_alnum_day = dict((v,k) for k,v in alnum_day_num_day.items())\n",
    "\n",
    "month_abb = {}\n",
    "month_to_num = {}\n",
    "for i in range(1,13):\n",
    "    month_abb[calendar.month_name[i].lower()] = calendar.month_abbr[i].lower()\n",
    "    month_to_num[calendar.month_name[i].lower()] = i\n",
    "    \n",
    "\n",
    "def num_to_text(num:int,remove_sw:bool=True)->str:\n",
    "    '''\n",
    "    Change a number to words\n",
    "    '''\n",
    "    result = engine.number_to_words(num)\n",
    "    if remove_sw:\n",
    "        result = result.replace(' and ',' ')\n",
    "    result = re.sub(r\"[^a-z]\",' ',result)\n",
    "    return re.sub(r\"\\s+\",' ',result)\n",
    "    \n",
    "\n",
    "def text2num(textnum:str, numwords:dict={})->int:\n",
    "    '''\n",
    "    Method to convert the TEXT format number to proper integer\n",
    "    '''\n",
    "    if not numwords:\n",
    "      units = [\n",
    "        \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
    "      ]\n",
    "\n",
    "      tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "      scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
    "\n",
    "      numwords[\"and\"] = (1, 0)\n",
    "      for idx, word in enumerate(units):    numwords[word] = (1, idx)\n",
    "      for idx, word in enumerate(tens):     numwords[word] = (1, idx * 10)\n",
    "      for idx, word in enumerate(scales):   numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
    "\n",
    "    current = result = 0\n",
    "    for word in textnum.split():\n",
    "        if word not in numwords:\n",
    "          raise Exception(\"Illegal word: \" + word)\n",
    "\n",
    "        scale, increment = numwords[word]\n",
    "        current = current * scale + increment\n",
    "        if scale > 100:\n",
    "            result += current\n",
    "            current = 0\n",
    "\n",
    "    return result + current\n",
    "\n",
    "\n",
    "\n",
    "def is_ascii(s:str)->bool:\n",
    "    '''\n",
    "    Method to return is a given token is ASCII or not\n",
    "    '''\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "\n",
    "def is_date(s:str)->bool:\n",
    "    '''\n",
    "    Method to return if a given string is in date 12.08.2020 format or not\n",
    "    '''\n",
    "    try:\n",
    "        sum(int(x) for x in s.strip().split('.'))\n",
    "        return True\n",
    "    except ValueError as e:\n",
    "        return False\n",
    "\n",
    "def clean(text):\n",
    "    text=text.replace('/','.')\n",
    "    text=text.replace('\\\\','.')\n",
    "    text=text.replace('. ','.')\n",
    "    text=text.replace(', ','')\n",
    "    text=text.replace(',','')\n",
    "    text=text.replace('-','.')\n",
    "    text=text.replace('/','.')\n",
    "    text=text.replace('. ','.')\n",
    "    #text=text.lower()\n",
    "    return(text)\n",
    "\n",
    "    \n",
    "PROBLEM = 'restore original index value  and length after preprocessing'\n",
    "def preprocess(s:[bytes,str],lowercase:bool=False,remove_all_spec:bool=True)->str:\n",
    "    if isinstance(s, (bytes)):\n",
    "        s = s.decode('utf-8')\n",
    "        \n",
    "    if lowercase:\n",
    "        s = s.lower()\n",
    "    \n",
    "    s = s.replace('thousands','thousand')\n",
    "    s = s.replace('hundreds','hundred')\n",
    "    \n",
    "    if remove_all_spec:\n",
    "        s = re.sub(r\"[^a-zA-Z0-9]\",' ',s)\n",
    "    else:\n",
    "        s = re.sub(r\"[^/.,a-zA-Z0-9]\",' ',s) # as there are  only 3 special signs used in the training csv\n",
    "        \n",
    "    s = re.sub(r\"\\s+\",\" \",s)\n",
    "    \n",
    "    dummy_list = []\n",
    "    for token in s.split(' '):\n",
    "        if (token.strip().isascii() if ver_37 else is_ascii(token.strip())):\n",
    "            dummy_list.append(token.strip())\n",
    "    return ' '.join(dummy_list)\n",
    "\n",
    "\n",
    "def find_year_end(text:str,y:str)->int:\n",
    "    '''\n",
    "    Give the matching year from a string\n",
    "    '''\n",
    "    if text.find(y)!=-1: # find year if it's directly 2020\n",
    "        return text.find(y)+4\n",
    "    else: #if its two thousand twenty in the text: check directly directly\n",
    "        y_text = num_to_text(y)\n",
    "        end = text.find(y_text)\n",
    "        if end !=-1:\n",
    "            return end+len(y_text)\n",
    "        else:\n",
    "            y_text_last = y_text.split(' ')[-1] # get last token\n",
    "            end = text.find(y_text_last)\n",
    "            if end!=-1:\n",
    "                return end+len(y_text_last)\n",
    "            else:\n",
    "                return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input: Create Training Data Format\n",
    "Train is used for training the model. Model will look at the data points of in train data set and depending on the task, data type and cost function, it'll try to adjust the weight metrices accordingly to minimize the loss function. Gradients will flow back into network according to the loss generated in train data. Validation data set is used to test the performance of model per epoch during the training but validation data does not change the weights of matrices so there is no gradient flow during the validation phase. Testing data is a real world depiction of actual data which we will get during deployment. We do not use testing data until we have the model fully trained. it is there to check the efficiency ofmertic defined for the model\n",
    "\n",
    "So data format is different for different models. For spaCy, it is in the form of `SPACY DATA FORMAT` and BERT type of models expect a different type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_data_format(doc_files_path:str='.data//Training_data/',csv_path:str='./data/TrainingTestSet.csv')->list:\n",
    "    walk= list(os.walk(doc_files_path))\n",
    "    dir_files = walk[0][2]\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    train_data = []\n",
    "    ent_names = df.columns.tolist()[1:]\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    for i,index in enumerate(df.index.tolist()):\n",
    "        row_data = df.iloc[i,:].values.tolist()\n",
    "        entities = {\"entities\":[]}\n",
    "\n",
    "        for file in dir_files: # get each and every file name\n",
    "            if file.split('.')[0] == row_data[0]:\n",
    "                text = preprocess(textract.process(doc_files_path+file))\n",
    "\n",
    "                for j,entry in enumerate(row_data[1:]):\n",
    "                    if not pd.isna(entry): # if it is not Null value\n",
    "\n",
    "                        if isinstance(entry,str) and not is_date(entry): # if it is NAME only party1 party2\n",
    "                            entry = entry.strip()\n",
    "                            start = text.find(entry)\n",
    "                            if start!=-1: # if it a straight match\n",
    "                                entities['entities'].append((start,start+len(entry),ent_names[j]))\n",
    "\n",
    "                            else: # remove all the spaces and special characters from the csv data and check string again\n",
    "                                entry = re.sub(r\"[^a-zA-Z0-9]\",' ',entry)\n",
    "                                entry = re.sub(r\"\\s+\",' ',entry) # because csv can have Mr. mrs. etc\n",
    "                                start = text.find(entry)\n",
    "                                if start!=-1:\n",
    "                                    entities['entities'].append((start,start+len(entry),ent_names[j]))\n",
    "\n",
    "                                else: # split the name. check first and last token and match\n",
    "                                    e = entry.split(' ')\n",
    "                                    if len(e)>1: # if more than 2 tokens\n",
    "                                        start = text.find(e[0]) # starts where first token of name starts\n",
    "                                        end = text.find(e[-1])+len(e[-1]) # ends where last token of name starts + length of last token\n",
    "                                        entities['entities'].append((start,end,ent_names[j]))\n",
    "\n",
    "\n",
    "\n",
    "                        elif isinstance(entry,np.float64): # If it is Aggrement Value or Renewal Notice\n",
    "                            entry = str(int(entry))\n",
    "                            start = text.find(entry)\n",
    "                            if start!= -1: # if found directly 12345 etc\n",
    "                                entities['entities'].append((start,start+len(entry),ent_names[j]))\n",
    "                            else: # if it is in words like two thousand three\n",
    "                                n2w = num2words(int(entry)).replace(',','')\n",
    "                                start = text.find(n2w)\n",
    "                                if start!=-1:\n",
    "                                    entities['entities'].append((start,start+len(n2w),ent_names[j]))\n",
    "\n",
    "\n",
    "                        elif is_date(entry): #  it can either be 09, 9th or nineth\n",
    "                            entry = re.sub(r\"[^a-zA-Z0-9]\",' ',entry)\n",
    "                            entry = re.sub(r\"\\s+\",' ',entry) # because in the training data, we have removed . so there is no chance of exact match\n",
    "                            start = text.find(entry)  # if exactly 02 02 2020\n",
    "                            if start!=-1:\n",
    "                                entities['entities'].append((start,start+len(entry),ent_names[j]))\n",
    "                            else: # if it is 09  or nineth or 9th\n",
    "                                e = entry.split(' ')\n",
    "                                d,m,y = e[0],e[1],e[2] # split day month year\n",
    "\n",
    "                                if text.find(d)!=-1:# if it is 09\n",
    "                                    start = text.find(d)\n",
    "                                    end = find_year_end(text,y)\n",
    "                                    if start!=-1 and end!=-1 and end>start+8 and end-start<50:\n",
    "                                        entities['entities'].append((start,end,ent_names[j]))\n",
    "\n",
    "                                elif d in num_day_alnum_day: # if it is nineth\n",
    "                                    d = num_day_alnum_day[d]\n",
    "                                    start = text.find(d)\n",
    "                                    end = find_year_end(text,y)\n",
    "                                    if start!=-1 and end!=-1 and end>start+8 and end-start<50:\n",
    "                                        entities['entities'].append((start,end,ent_names[j]))\n",
    "\n",
    "                                elif d in alnum_day_al_day: # if it is 9th\n",
    "                                    d = num_day_alnum_day[d]\n",
    "                                    start = text.find(d)\n",
    "                                    end = find_year_end(text,y)\n",
    "                                    if start!=-1 and end!=-1 and end>start+8 and end-start<50:\n",
    "                                        entities['entities'].append((start,end,ent_names[j]))\n",
    "\n",
    "                train_data.append((text,entities))\n",
    "                break\n",
    "    return train_data\n",
    "\n",
    "\n",
    "\n",
    "def spaCy_train_data(doc_files_path:str='./data/Training_data/',csv_path:str='./data/TrainingTestSet.csv')->list:\n",
    "    data=[]\n",
    "    train_labels = pd.read_csv(csv_path)\n",
    "    data_index=list(train_labels.columns)\n",
    "    count=0\n",
    "    new_data=[]\n",
    "    for i in os.listdir(doc_files_path):\n",
    "        doc = docx.Document(doc_files_path+i)  # Creating word reader object.\n",
    "        paragraph=doc.paragraphs\n",
    "        st=\"\"\n",
    "        for j in paragraph:\n",
    "            st+=clean(str(j.text))+\" \"\n",
    "        data.append(st)\n",
    "        temp=train_labels.loc[train_labels['File Name'] == i[:-9]]\n",
    "\n",
    "        entities=[]\n",
    "        for col in data_index[1:]:\n",
    "            query=str(tuple(temp[col])[0])\n",
    "\n",
    "            if(col=='Aggrement Start Date' or col=='Aggrement End Date'):\n",
    "                if(st.find(query)!=-1):\n",
    "                    entities.append([st.find(query),st.find(query)+len(query),col])\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "            if(type(tuple(temp[col])[0])==type(8.0) and tuple(temp[col])[0]>=0): \n",
    "                    query=str(int(tuple(temp[col])[0]))      \n",
    "            if(st.find(query)!=-1 or st.lower().find(query.lower())!=-1):\n",
    "                    entities.append([st.find(query),st.find(query)+len(query),col])\n",
    "\n",
    "            else:\n",
    "                count+=1\n",
    "        new_data.append((st,{'entities':entities}))\n",
    "    return new_data\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SaCy Model Training\n",
    "Training for just 1o epochs. Increase to 100 to see the loss going ALMOST 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of Epoch : 0 Training Loss :  6572.982368014429\n",
      "End of Epoch : 1 Training Loss :  45.579438706202104\n",
      "End of Epoch : 2 Training Loss :  160.9364908095085\n",
      "End of Epoch : 3 Training Loss :  328.88992184182916\n",
      "End of Epoch : 4 Training Loss :  226.82325800525746\n",
      "End of Epoch : 5 Training Loss :  36.75989285803055\n",
      "End of Epoch : 6 Training Loss :  28.35338048104185\n",
      "End of Epoch : 7 Training Loss :  30.24545299475947\n",
      "End of Epoch : 8 Training Loss :  103.38894767309104\n",
      "End of Epoch : 9 Training Loss :  40.662100484302925\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = spaCy_train_data()\n",
    "\n",
    "nlp = spacy.blank('en') # get the nlp model in memory\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner') # instantiate a Pipeline\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "\n",
    "    \n",
    "for _, labels in TRAIN_DATA:\n",
    "     for ent in labels.get('entities'): # get the entities \n",
    "        ner.add_label(ent[2]) # add label\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] # Get all the pipe names Except NER\n",
    "count=0\n",
    "loss_val=[]\n",
    "with nlp.disable_pipes(*other_pipes):  # keep only the NER model\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(epochs):\n",
    "        random.shuffle(TRAIN_DATA) # shiuff\n",
    "        losses = {}\n",
    "        \n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            try:                \n",
    "                nlp.update(\n",
    "                    [text], \n",
    "                    [annotations], \n",
    "                    drop=0.35, \n",
    "                    sgd=optimizer,\n",
    "                    losses=losses)\n",
    "            except:\n",
    "                pass\n",
    "                count=count+1\n",
    "        loss_val.append(list(losses.values())[0])\n",
    "        print(\"End of Epoch :\",i,\"Training Loss : \",list(losses.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Methods\n",
    "1. [Bi-LSTM WITHOUT CRF](https://www.depends-on-the-definition.com/interpretable-named-entity-recognition/)\n",
    "2. [LSTM + CNN without CRF](https://github.com/kamalkraj/Named-Entity-Recognition-with-Bidirectional-LSTM-CNNs/blob/master/nn.py)\n",
    "3. [Bi-LSTM + CRF](https://confusedcoders.com/data-science/deep-learning/how-to-build-deep-neural-network-for-custom-ner-with-keras)\n",
    "4. [Bi-LSTM WITHOUT CRF WITHOUT Embeddings](https://blog.codecentric.de/en/2020/11/take-control-of-named-entity-recognition-with-you-own-keras-model/)\n",
    "5. [CRF based: sklearn and Keras](https://blog.codecentric.de/en/2020/11/take-control-of-named-entity-recognition-with-you-own-keras-model/)\n",
    "6. [ELMO Embeddings without CRF](https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede)\n",
    "8. [Using Fast Text without Gensim](https://www.kaggle.com/vsmolyakov/keras-cnn-with-fasttext-embeddings)\n",
    "9. [GloVe, ParaGram, FastText](https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing)\n",
    "10. [GoogleNews, WikiNews Embeddings](https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings)\n",
    "11. [Gensim Word2Vec](https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings)\n",
    "12. [Different Embeddings using Different libraries](https://towardsdatascience.com/word-embeddings-in-2020-review-with-code-examples-11eb39a1ee6d)\n",
    "13. [Create and load own FastText, Word2Vec using Gensim](https://sturzamihai.com/how-to-use-pre-trained-word-vectors-with-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER from Scratch using Keras\n",
    "We'll be using different ways to do this work in Keras. Please read all the comments and docstrings so that you can know what each function is doing. I have compiled many different methods inside a function so that it is looks like Swiss knife of Keras based NER. We'll be looking at \n",
    "1. Basic NER like a classification task\n",
    "2. NER using CRF\n",
    "3. Using above methods with Different Embeddings \n",
    "4. Preprocess data for NER\n",
    "5. How to create and load different types of Embeddings in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-geo', 'B-tim', 'B-org', 'I-per', 'B-per', 'I-org', 'B-gpe', 'I-geo', 'I-tim', 'B-art', 'B-eve', 'I-art', 'I-eve', 'B-nat', 'I-gpe', 'I-nat'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/ner_dataset.csv',encoding=\"ISO-8859-1\") # encoding param is must as it'll give errors\n",
    "print(df['Tag'].value_counts().index.tolist(),'\\n')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `Sentence` is the sentence in which all the Words belong to. Each sentence has different words. `Word` is an individual word which belongs to a particulat sentence like `Thousands, of` belong to `Sentence 1`. `POS` is the telling us whether the word is Verb, Object Noun or some other. `Tag` is giving us information about what kind of word it is. IS it `Date`, `Place` `Name` etc etc.  So `O` means it is `Other` and does not mean anything, `B-org`: Beginning of Organisation (Ex United), `I-org` means Intermediate of Organisation (Health and Group). So `United Health Group` can be seen as `B-org,I-org,I-org`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing (Changing Structure)\n",
    "\n",
    "In most of the NLP tasks, we have to do some certain steps:\n",
    "1. Get data in `['text 1 here', 'here comes text 2', 'these can be short or long', 'these can be whole documents too']` format.\n",
    "2. Clean and Tokenize each sentence. Tokenization means breaking sentencess in tokens (mostly words) based on some criteria (mostly blank space).\n",
    "3. Create Vocablury from sentences. You can make the vocab from all words or you can chose maximum number of words to avoid huge dimensions.\n",
    "4. Give each word in vocab a unique number because machines know numbers only.\n",
    "5. Set a maximum length to use so that you can represent variable length sentences by using truncating or padding. If sentences are shorter than that number, add a padding token and if they are larger, truncate them.\n",
    "6. After doing this, you'll have your data ready to be processed by NN and this data will be something like:\n",
    "\n",
    "`\n",
    "['hello my name is', \n",
    "'what?',\n",
    "'what is slim shady anyway?']\n",
    "`\n",
    "\n",
    "will become :\n",
    "\n",
    "`\n",
    "[[1,2,3,4], # max_len = 4 \n",
    " [5,0,0,0], # padding = 'post'\n",
    " [5,4,6,7]] # 'what' has been given number 5. 'anyway' has been truncated because length > 4\n",
    "`\n",
    "Again, these numbers will be converted as `one_hot(number)` and then it'll go to the the `Embedding` Layer. To know what are embeddings and how can they be used, [Check this Notebook out](my_NLP_Kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning & Changing Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1  Sentence: 1             of   IN   O\n",
       "2  Sentence: 1  demonstrators  NNS   O"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(method=\"ffill\",inplace=True) # Fill all the NaN values with the previous seen. DO NOT USE THIS WITHOUT LOOKING AT THE DATA> I did it for SENTENCE\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 35179 unique words in our data\n",
      "\n",
      "We have 17 unique Tags in our data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = list(set(df[\"Word\"].values)) # Get each UNIQUE word (Our vocab length basically)\n",
    "words.append(\"ENDPAD\") # Add a EndPad as extra so that you can append it ot the existing sentence. Will be used with max_len\n",
    "n_words = len(words) \n",
    "print(f\"We have {n_words} unique words in our data\\n\") # vocab size\n",
    "\n",
    "tags = list(set(df[\"Tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(f\"We have {n_tags} unique Tags in our data\\n\") # Number of Unique Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "    '''\n",
    "    A very famous and most used class to change the shape of data for NER tasks. Check any NEW post from\n",
    "    https://www.depends-on-the-definition.com/tags/nlp\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        '''\n",
    "        args:\n",
    "            data: DataFrame which has exactly 3 Columns by name as Word, POS and Tag\n",
    "        '''\n",
    "        self.n_sent = 1 # Number of sentence\n",
    "        self.data = data # your dataframe\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())] # DataFrame Specific grouping\n",
    "        \n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "        \n",
    "    \n",
    "    def get_next(self): # Works like a generator You can't access anything else\n",
    "        '''\n",
    "        Method to get entities per Sentence. Results will be in [(word1,pos1,tag1),(word2,pos2,tag2)....]    \n",
    "        '''\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Method to return how many unique Sentences we have in total.Use it with len(object) \n",
    "        '''\n",
    "        return self.sentences\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        When you use print(object), this line will be printed. Just another \"DUNDER\" method\n",
    "        '''\n",
    "        return f\"Object of type {type(self)}. Can't print the whole data. Make your own Function\"\n",
    "    \n",
    "    \n",
    "    \n",
    "class PreprocessEmbedding():\n",
    "    '''\n",
    "    Default Preprocessing method for most of the NLP task for Text.\n",
    "    Support any .vec, .txt or .bin files from popular Embeddings like GloVe, Word2Vec, Fasttext, Paragram \n",
    "    '''\n",
    "    # check: https://stackabuse.com/pythons-classmethod-and-staticmethod-explained/ for @staticmethod use\n",
    "    \n",
    "    @staticmethod # You don't have to create an object of this class in order access this method. Preprocess.preprocess_data()\n",
    "    def preprocess_data(data:list,max_length:int):\n",
    "        '''\n",
    "        Method to parse, tokenize, build vocab and padding the text data\n",
    "        args:\n",
    "            data: List of all the texts as: ['this is text 1','this is text 2 of different length']\n",
    "            max_length: maximum length to consider for an individual text entry in data\n",
    "        out:\n",
    "            vocab size, fitted tokenizer object, encoded input text and padded input text\n",
    "        '''\n",
    "        tokenizer = Tokenizer() # set num_words, oov_token arguments depending on your usecase\n",
    "        tokenizer.fit_on_texts(data)\n",
    "        vocab_size = len(tokenizer.word_index) + 1 # extra 1 for unknown words which will be all 0s when loading pre trained embeddings\n",
    "        encoded_docs = tokenizer.texts_to_sequences(data)\n",
    "        padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post') \n",
    "        # padding = 'post' means that append 0s at the end if sentence length is less than max_length. check: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "        return vocab_size,tokenizer,encoded_docs,padded_docs\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pretrained_embeddings(fitted_tokenizer,vocab_size:int,emb_file:str,emb_dim:int=300,):\n",
    "        '''\n",
    "        All 300D Embeddings: https://www.kaggle.com/reppy4620/embeddings \n",
    "        '''\n",
    "        if '.bin' in emb_file: # if it is binary file, it is not embeddings but the MODEL itself. It could be fasttext or word2vec model\n",
    "            model = KeyedVectors.load_word2vec_format(emb_file, binary=True)\n",
    "            # emb_file = emb_file.replace('.bin','.txt') # general purpose path\n",
    "            emb_file = './new_emb_file.txt' # for Kaggle because you have to save data in out dir only\n",
    "        model.save_word2vec_format(emb_file, binary=False)\n",
    "    \n",
    "        # open and read the contents of the .txt / .vec file (.vec is same as .txt file)\n",
    "        embeddings_index = dict() \n",
    "        with open(emb_file,encoding=\"utf8\",errors='ignore') as f:\n",
    "            for i,line in enumerate(f): # each line is as: hello 0.9 0.3 0.5 0.01 0.001 ...\n",
    "                if i>0: # why this? You'll see in most of the Kaggle Kernals as if len(line)>100. It is because there is a difference between GloVe style and Word2Vec style embeddings\n",
    "                    # check this link: https://radimrehurek.com/gensim/scripts/glove2word2vec.html\n",
    "\n",
    "                    values = line.split(' ') \n",
    "                    word = values[0] # first value is \"hello\" \n",
    "                    coefs = np.asarray(values[1:], dtype='float32') # everything else is vector of \"hello\"\n",
    "                    embeddings_index[word] = coefs\n",
    "\n",
    "        # create the embedding matrix or Embedding weights based on your vocab\n",
    "        embedding_matrix = np.zeros((vocab_size, emb_dim)) # build embeddings based on our vocab size\n",
    "        for word, i in fitted_tokenizer.word_index.items(): # get each vocab token one by one\n",
    "            embedding_vector = embeddings_index.get(word) # get from loaded embeddings\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector # if it is present, just replace the corresponding vectors\n",
    "\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'NNS', 'O'), ('of', 'IN', 'O'), ('demonstrators', 'NNS', 'O'), ('have', 'VBP', 'O'), ('marched', 'VBN', 'O'), ('through', 'IN', 'O'), ('London', 'NNP', 'B-geo'), ('to', 'TO', 'O'), ('protest', 'VB', 'O'), ('the', 'DT', 'O'), ('war', 'NN', 'O'), ('in', 'IN', 'O'), ('Iraq', 'NNP', 'B-geo'), ('and', 'CC', 'O'), ('demand', 'VB', 'O'), ('the', 'DT', 'O'), ('withdrawal', 'NN', 'O'), ('of', 'IN', 'O'), ('British', 'JJ', 'B-gpe'), ('troops', 'NNS', 'O'), ('from', 'IN', 'O'), ('that', 'DT', 'O'), ('country', 'NN', 'O'), ('.', '.', 'O')] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(df) # It'll be a generator so you have to use get_next() method\n",
    "print(getter.get_next(),'\\n\\n') # See a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London has id: 15055 and B-org has id: 9\n",
      "\n",
      "Sample X: [ 5697 29980 33667 34099 11026 13928 15055 26265  3264  7645 18779  5350\n",
      "  4648 14165 10656  7645 29617 29980  3317  2598 32058 27817 22849 35153\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "\n",
      "Sample y SPARSE: [16, 16, 16, 16, 16, 16, 5, 16, 16, 16, 16, 16, 5, 16, 16, 16, 16, 16, 7, 16, 16, 16, 16, 16]\n"
     ]
    }
   ],
   "source": [
    "max_len = 75 # Maximum Length of a Sentence\n",
    "\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words)} # Make a dict {word1:1, word2:2, ....}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)} # Dict of {tag1:1, tag2:2, .....}\n",
    "print(f'''London has id: {word2idx[\"London\"]} and B-org has id: {tag2idx['B-org']}\\n''')\n",
    "\n",
    "sentences = getter.sentences\n",
    "X = [[word2idx[w[0]] for w in s] for s in sentences] # Words converted as Numbers\n",
    "y = [[tag2idx[w[2]] for w in s] for s in sentences] # Tags converted as numbers\n",
    "\n",
    "\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=0) # Padding as '0' in the end\n",
    "y_sparse = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"]) # Padding a 'O'= Other\n",
    "\n",
    "y_categorical = [to_categorical(i, num_classes=n_tags) for i in y_sparse] # Change it to One Hot Encoded Values (Categorical Cross Entropy)\n",
    "\n",
    "print(f\"Sample X: {X[0]}\\n\\nSample y SPARSE: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Custom Layers\n",
    "The model we're trying to build below is (Almost) Everything for the price of one. So in order to use sone things, we need to build those things as they are not given by default. We'll be building [ELMO](https://stackoverflow.com/questions/53798582/is-elmo-a-word-embedding-or-a-sentence-embedding) and [Attention](https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39) Layers. Two types of Attentions are also given in [Tensorflow's official Repo](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) but we'll use a very simple model.\n",
    "\n",
    "<font color=\"red\">**NOTE: Elmo Embeddings are not working with this code. I have asked for help from community to integrate  it with `tf 2`. Will update when it'll be usable**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "batch_size = 32\n",
    "\n",
    "# Another Implementation@: https://sujayskumar.com/2018/10/02/elmo-embeddings-in-keras/\n",
    "# About ELMO @: https://stackoverflow.com/questions/53798582/is-elmo-a-word-embedding-or-a-sentence-embedding\n",
    "\n",
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x,tf.string)),\n",
    "                              \"sequence_len\": tf.constant(batch_size*[max_len])},\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]\n",
    "\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    '''\n",
    "    Custom Attention Layer. It is dot product or Luong Style Attention. \n",
    "    Code from: @ https://stackoverflow.com/questions/62948332/how-to-add-attention-layer-to-a-bi-lstm/62949137#62949137\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(Attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size:int,n_tags:int,max_len:int,emb_dim:int=300,emb_weights=False,use_attention:bool=True,\n",
    "                use_elmo:bool=False,use_crf:bool=False,train_embedding:bool=False):\n",
    "    '''\n",
    "    Build and return a Keras model based on the given inputs\n",
    "    args:\n",
    "        n_tags: No of unique 'y' tags present in the data\n",
    "        max_len: Maximum length of sentence to use\n",
    "        emb_dim: Size of embedding dimension\n",
    "        emb_weights: pretrained Embedding Weights for Embedding Layer. if False, use default\n",
    "        use_attention: Whether to use the Attentiom Layer ot not\n",
    "        use_elmo: Whether to use Elmo Embeddings\n",
    "        use_crf: Whether to use the CRF layer\n",
    "        train_embedding: Whether to train the embeddings weights\n",
    "    out:\n",
    "        Keras model. See comments for each type of loss function and metric to use\n",
    "    '''\n",
    "    assert not(isinstance(emb_weights,np.ndarray) and  use_elmo), \"Either provide embedding weights or use ELMO. Not both\"\n",
    "    \n",
    "    inputs = Input(shape=(max_len,))\n",
    "    \n",
    "    if isinstance(emb_weights,np.ndarray):\n",
    "        x = Embedding(trainable=train_embedding,input_dim=vocab_size, output_dim=emb_dim, input_length=max_len, mask_zero=True, embeddings_initializer=keras.initializers.Constant(emb_weights))(inputs)\n",
    "    elif use_elmo:\n",
    "        x = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(inputs) # Lambda will create a layer based on the function defined  \n",
    "    else: # use default Embeddings\n",
    "        x = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_len, mask_zero=True,)(inputs) # n_words = vocab_size\n",
    "    \n",
    "    x = Bidirectional(LSTM(units=50, return_sequences=True,recurrent_dropout=0.1))(x)\n",
    "    \n",
    "    if use_attention:\n",
    "        x = Attention(return_sequences=True)(x) # receives 3-D and Outputs 3-D because (in this case only) Dense needs to work with 3-D in & out\n",
    "    \n",
    "    if use_crf: \n",
    "        output = Dense(n_tags, activation=None)(x)\n",
    "        crf = CRF(dtype='float32') # it does not take any n_tags. See the documentation.\n",
    "        output = crf(output)\n",
    "        base_model = Model(inputs, output)\n",
    "        model = ModelWithCRFLoss(base_model) # It has Loss and Metric already. Change the model if you want to use DiceLoss.\n",
    "        return model # Do not use any metric or loss with this model.compile(). Just use Optimizer and run training\n",
    "\n",
    "    else:\n",
    "        out = Dense(n_tags, activation=\"softmax\")(x) # Wrap it around TimeDistributed(Dense()) if you have old versions\n",
    "        model = Model(inputs, out)\n",
    "        return model # use \"sparse_categorical_crossentropy\", \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "### Dense + Categorical Cross Entropy + Attention + `NO CRF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - 195s 203ms/step - loss: 0.0883 - accuracy: 0.9288 - val_loss: 0.0363 - val_accuracy: 0.9635\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 # just for testing\n",
    "\n",
    "# Use this for simple use case. It is on CATEGORICAL Y\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y_categorical, test_size=0.2,random_state=SEED) \n",
    "\n",
    "model = build_model(vocab_size=n_words+1,n_tags=n_tags,max_len=max_len,use_attention=True,use_crf=False)\n",
    "model.compile('adam','categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_tr, np.array(y_tr), batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRF + Attention + CRF Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960/960 [==============================] - 210s 218ms/step - crf_loss: 88.8722 - accuracy: 0.8611 - val_crf_loss_val: 25.2340 - val_val_accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "# CRF type Y labels. It is on SPARSE Y\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y_sparse, test_size=0.2,random_state=SEED) # Use this for use_crf = True\n",
    "\n",
    "model = build_model(vocab_size=n_words+1,n_tags=n_tags,max_len=max_len,use_attention=True,use_crf=True)\n",
    "model.compile('adam') # See build_model() and if crf: block. Model is wrapped within model itself\n",
    "\n",
    "history = model.fit(X_tr, np.array(y_tr), batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
